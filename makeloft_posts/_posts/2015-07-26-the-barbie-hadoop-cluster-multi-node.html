---
layout: post
title: The Barbie Hadoop Cluster (Multi-Node Cluster)
date: '2015-07-26T18:32:00.000-07:00'
author: Will Stedden
tags:
- DataScience
- IT
modified_time: '2015-08-21T17:56:12.849-07:00'
thumbnail: http://2.bp.blogspot.com/-XGwAxKreBtw/VbWKR9SptdI/AAAAAAAAEHU/FgdjpgyQkKM/s72-c/CAM00240.jpg
blogger_id: tag:blogger.com,1999:blog-40405577383217050.post-6367655432440764070
blogger_orig_url: http://www.makeloft.org/2015/07/the-barbie-hadoop-cluster-multi-node.html
---

<div dir="ltr" style="text-align: left;" trbidi="on"><p>I've finally finished the Barbie Hadoop Cluster!  It's been several months since I <a href="/2015/01/the-barbie-hadoop-cluster-stage-1.html">started the project</a>, but after the hiatus I was ready to come back and get it finished.</p><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-XGwAxKreBtw/VbWKR9SptdI/AAAAAAAAEHU/FgdjpgyQkKM/s1600/CAM00240.jpg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-XGwAxKreBtw/VbWKR9SptdI/AAAAAAAAEHU/FgdjpgyQkKM/s400/CAM00240.jpg" /></a></div><p>Due to space limitations in our apartment, the stack of 9 Dell PCs is also being used as a TV/monitor stand.  They're connected through a switch and all the extra ports on my roouter at the moment.  Hopefully at <a href="http://www.artificechicago.org/">her permanent home</a>, Barbie will look a little more at ease.</p>  <h4>Setting Barbie Up</h4><p>The beginning of the procedure for the setup is basically exactly the same as I described in my <a href="http://www.makeloft.org/2015/01/the-barbie-hadoop-cluster-stage-1.html">previous post</a>, except for a change in the configuration files in the <a href="https://github.com/lots-of-things/hadoop-compiled">associated github project</a>. Pretty much the entirety of the hadoop setup on each computer was accomplished simply with the folowing command.<p> <pre><br />sudo git clone https://github.com/lots-of-things/hadoop-compiled.git /usr/local/hadoop<br /></pre><p>This is followed by changing permissions and adding a few directories to the $PATH by modifiying .bashrc.<p><pre><br />sudo chown -R doopy:hadoop /usr/local/hadoop<br />cat /usr/local/addToBash >> ~/.bashrc<br /></pre> <p>This had to be done on each of the seven working machines.  They each had different RAM and hard disk amounts, with skipper and stacie being the best and barbie and ken being the worst.</p><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-qNgeAOJanSY/VbWUclEdoeI/AAAAAAAAEHw/WLnnP2yBb70/s1600/CAM00241.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://3.bp.blogspot.com/-qNgeAOJanSY/VbWUclEdoeI/AAAAAAAAEHw/WLnnP2yBb70/s400/CAM00241.png" /></a></div><p>I changed the /etc/hosts on every machine so that each machine could recognize each other by name.</p><pre><br />10.0.0.9        barbie<br />10.0.0.10       kelly<br />10.0.0.13       skipper<br />10.0.0.14       stacie<br />10.0.0.16       ken<br />10.0.0.17       christie<br />10.0.0.18       midge<br /></pre> <p>I decided to make skipper the master, Each machine needs to let the master connect by ssh without a password so I copied Skipper's rsa key made in the initial setup using </p><pre><br />ssh-copy-id -i ~/.ssh/id_rsa.pub remote-host<br /></pre><p>Where remote-host had to be changed to every machine I wanted to connect to. Finally, skipper's slaves file (etc/hadoop/slaves) needed to have each machine added to it (including herself so she would do some work too instead of just running as the NameNode).</p><pre><br />barbie<br />kelly<br />skipper<br />stacie<br />ken<br />christie<br />midge<br /></pre><h4>In all honesty...</h4><p>I screwed this up about a dozen times before getting it to work, but every mistake was something well-documented with solutions on the web.  The best trick I learned was to delete the datanode temporary file on each machine whenever I screwed things up.  In my setup that would be done like this.</p><pre><br />rm -r /usr/local/hadoop/datanode<br /></pre> <h4>And Finally</h4><p>I was finally able to load a few documents into the hdfs and run the wordcount on them across all machines.  Overall, I feel extremely satisfied getting this done finally (though I've got a much more <a href="http://spark.apache.org/">sophisticated system</a> at my disposal at the cluster <a href="https://rcc.uchicago.edu/">where I work</a>). Now I'm just getting ready to try it out on one of my pet projects!</p><br /></div>